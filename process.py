import os
import pandas as pd
import json
import re
from pathlib import Path
import logging
from datetime import datetime
import fitz  # PyMuPDF
import zipfile
import olefile
import struct

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('document_conversion.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class DocumentConverter:
    def __init__(self, input_path, output_path):
        self.input_path = Path(input_path)
        self.output_path = Path(output_path)
        self.logs_path = Path("conversion_logs")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_path.mkdir(exist_ok=True)
        self.logs_path.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        self.conversion_results = {
            'total_files': 0,
            'successful_files': [],
            'failed_files': [],
            'short_files': [],
            'statistics': {}
        }
    
    def clean_text(self, text, filename):
        """í…ìŠ¤íŠ¸ ì •ì œ ë° ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # íŒŒì¼ëª… ë§ˆí‚¹ ì¶”ê°€
        cleaned_text = f"[ë¬¸ì„œ: {filename}]\n\n"
        
        # ê¸°ë³¸ ì •ì œ
        text = re.sub(r'\s+', ' ', text)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\n\s*\n', '\n\n', text)  # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        
        # êµ¬ì¡° íƒœê·¸ ì œê±° ë˜ëŠ” ë§ˆí‚¹
        text = re.sub(r'\[ì´ë¯¸ì§€\].*?\[/ì´ë¯¸ì§€\]', '[ì´ë¯¸ì§€ ìƒëµë¨]', text, flags=re.DOTALL)
        text = re.sub(r'\[í‘œ\].*?\[/í‘œ\]', '[í‘œ ìƒëµë¨]', text, flags=re.DOTALL)
        text = re.sub(r'\[ê·¸ë¦¼\].*?\[/ê·¸ë¦¼\]', '[ê·¸ë¦¼ ìƒëµë¨]', text, flags=re.DOTALL)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£.,!?;:()\[\]{}"\'-]', '', text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì •ì œ
        sentences = re.split(r'[.!?]+', text)
        cleaned_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 5:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
                cleaned_sentences.append(sentence)
        
        cleaned_text += '. '.join(cleaned_sentences) + '.'
        
        return cleaned_text.strip()
    
    def extract_pdf_text(self, pdf_path):
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            doc = fitz.open(pdf_path)
            text = ""
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                text += page_text + "\n"
            
            # ë¬¸ì„œë¥¼ ë‹«ê¸° ì „ì— í˜ì´ì§€ ìˆ˜ ì €ì¥
            page_count = len(doc)
            doc.close()
            
            return {
                'success': True,
                'text': text,
                'pages': page_count,
                'error': None
            }
            
        except Exception as e:
            return {
                'success': False,
                'text': '',
                'pages': 0,
                'error': str(e)
            }
    
    def extract_hwp_text_simple(self, hwp_path):
        """HWP íŒŒì¼ì—ì„œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OLE êµ¬ì¡° ê¸°ë°˜)"""
        try:
            if not olefile.isOleFile(hwp_path):
                return {
                    'success': False,
                    'text': '',
                    'pages': 0,
                    'error': 'ì˜¬ë°”ë¥¸ HWP íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.'
                }
            
            ole = olefile.OleFileIO(hwp_path)
            text = ""
            
            # HWP íŒŒì¼ì˜ ê¸°ë³¸ êµ¬ì¡°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            try:
                # DocInfo ìŠ¤íŠ¸ë¦¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
                if ole.exists('DocInfo'):
                    try:
                        doc_info = ole.openstream('DocInfo').read()
                        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
                        text += "ë¬¸ì„œ ì •ë³´: " + str(doc_info[:200]) + "\n"
                    except:
                        pass
                
                # BodyText ìŠ¤íŠ¸ë¦¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                if ole.exists('BodyText'):
                    try:
                        body_text = ole.openstream('BodyText').read()
                        # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        try:
                            # UTF-16ìœ¼ë¡œ ë””ì½”ë”© ì‹œë„
                            decoded_text = body_text.decode('utf-16', errors='ignore')
                            # í•œê¸€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                            korean_text = re.findall(r'[ê°€-í£\s]+', decoded_text)
                            text += ' '.join(korean_text)
                        except:
                            # ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                            try:
                                decoded_text = body_text.decode('cp949', errors='ignore')
                                korean_text = re.findall(r'[ê°€-í£\s]+', decoded_text)
                                text += ' '.join(korean_text)
                            except:
                                # ë°”ì´ë„ˆë¦¬ì—ì„œ ì§ì ‘ í•œê¸€ ì¶”ì¶œ ì‹œë„
                                try:
                                    # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í•œê¸€ ë¬¸ì íŒ¨í„´ ì°¾ê¸°
                                    korean_pattern = re.compile(b'[\x80-\xff]+')
                                    matches = korean_pattern.findall(body_text)
                                    for match in matches:
                                        try:
                                            decoded = match.decode('cp949', errors='ignore')
                                            if re.search(r'[ê°€-í£]', decoded):
                                                text += decoded + " "
                                        except:
                                            continue
                                except:
                                    text += "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
                    except Exception as stream_error:
                        text += f"ìŠ¤íŠ¸ë¦¼ ì½ê¸° ì‹¤íŒ¨: {str(stream_error)}"
                
                ole.close()
                
                if len(text.strip()) > 10:
                    return {
                        'success': True,
                        'text': text,
                        'pages': 1,  # HWPëŠ” í˜ì´ì§€ ìˆ˜ë¥¼ ì •í™•íˆ ì•Œê¸° ì–´ë ¤ì›€
                        'error': None
                    }
                else:
                    return {
                        'success': False,
                        'text': '',
                        'pages': 0,
                        'error': 'í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨'
                    }
                    
            except Exception as e:
                try:
                    ole.close()
                except:
                    pass
                return {
                    'success': False,
                    'text': '',
                    'pages': 0,
                    'error': f'HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}'
                }
                
        except Exception as e:
            return {
                'success': False,
                'text': '',
                'pages': 0,
                'error': f'HWP íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}'
            }
    
    def find_document_files(self):
        """ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°"""
        logging.info("=== ë¬¸ì„œ íŒŒì¼ íƒìƒ‰ ì‹œì‘ ===")
        
        pdf_files = list(self.input_path.rglob('*.pdf'))
        hwp_files = list(self.input_path.rglob('*.hwp'))
        
        all_files = pdf_files + hwp_files
        
        logging.info(f"ë°œê²¬ëœ íŒŒì¼ ìˆ˜:")
        logging.info(f"  PDF: {len(pdf_files)}ê°œ")
        logging.info(f"  HWP: {len(hwp_files)}ê°œ")
        logging.info(f"  ì´ê³„: {len(all_files)}ê°œ")
        
        return all_files
    
    def convert_documents(self):
        """ëª¨ë“  ë¬¸ì„œ ë³€í™˜"""
        logging.info("=== ë¬¸ì„œ ë³€í™˜ ì‹œì‘ ===")
        
        files = self.find_document_files()
        self.conversion_results['total_files'] = len(files)
        
        if not files:
            logging.info("ë³€í™˜í•  ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        for i, file_path in enumerate(files, 1):
            logging.info(f"ì²˜ë¦¬ ì¤‘ ({i}/{len(files)}): {file_path.name}")
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì²˜ë¦¬
            if file_path.suffix.lower() == '.pdf':
                result = self.extract_pdf_text(file_path)
            elif file_path.suffix.lower() == '.hwp':
                result = self.extract_hwp_text_simple(file_path)
            else:
                result = {
                    'success': False,
                    'text': '',
                    'pages': 0,
                    'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹'
                }
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            if result['success']:
                cleaned_text = self.clean_text(result['text'], file_path.name)
                result['cleaned_text'] = cleaned_text
                result['cleaned_length'] = len(cleaned_text)
            
            # ê²°ê³¼ ì²˜ë¦¬
            file_info = {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'file_type': file_path.suffix.lower(),
                'success': result['success'],
                'pages': result['pages'],
                'text_length': result.get('cleaned_length', 0),
                'error': result.get('error', None)
            }
            
            if result['success']:
                # ì„±ê³µí•œ ê²½ìš° í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
                output_file = self.output_path / f"{file_path.stem}.txt"
                try:
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(result['cleaned_text'])
                    
                    logging.info(f"  ì„±ê³µ: {result['pages']}í˜ì´ì§€, {result['cleaned_length']}ì")
                    
                    # ì§§ì€ íŒŒì¼ ì²´í¬
                    if result['cleaned_length'] <= 100:
                        self.conversion_results['short_files'].append(file_info)
                        logging.warning(f"  ê²½ê³ : í…ìŠ¤íŠ¸ê°€ ì§§ìŒ ({result['cleaned_length']}ì)")
                    
                    self.conversion_results['successful_files'].append(file_info)
                    
                except Exception as e:
                    logging.error(f"  íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
                    file_info['error'] = f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}"
                    self.conversion_results['failed_files'].append(file_info)
            else:
                logging.error(f"  ì‹¤íŒ¨: {result['error']}")
                self.conversion_results['failed_files'].append(file_info)
        
        # í†µê³„ ê³„ì‚°
        self._calculate_statistics()
        
        logging.info("=== ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ ===")
    
    def _calculate_statistics(self):
        """ë³€í™˜ í†µê³„ ê³„ì‚°"""
        successful_count = len(self.conversion_results['successful_files'])
        failed_count = len(self.conversion_results['failed_files'])
        short_count = len(self.conversion_results['short_files'])
        total_count = self.conversion_results['total_files']
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = (successful_count / total_count * 100) if total_count > 0 else 0
        
        # í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´ ê³„ì‚°
        text_lengths = [f['text_length'] for f in self.conversion_results['successful_files']]
        avg_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
        
        self.conversion_results['statistics'] = {
            'total_files': total_count,
            'successful_files': successful_count,
            'failed_files': failed_count,
            'short_files': short_count,
            'success_rate': success_rate,
            'average_text_length': avg_length,
            'min_text_length': min(text_lengths) if text_lengths else 0,
            'max_text_length': max(text_lengths) if text_lengths else 0
        }
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        logging.info("=== ê²°ê³¼ ì €ì¥ ===")
        
        # ì„±ê³µí•œ íŒŒì¼ ëª©ë¡ CSV ì €ì¥
        if self.conversion_results['successful_files']:
            successful_df = pd.DataFrame(self.conversion_results['successful_files'])
            successful_file = self.logs_path / 'successful_conversions.csv'
            successful_df.to_csv(successful_file, index=False, encoding='utf-8')
            logging.info(f"ì„±ê³µí•œ íŒŒì¼ ëª©ë¡ ì €ì¥: {successful_file}")
        
        # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ CSV ì €ì¥
        if self.conversion_results['failed_files']:
            failed_df = pd.DataFrame(self.conversion_results['failed_files'])
            failed_file = self.logs_path / 'failed_files.csv'
            failed_df.to_csv(failed_file, index=False, encoding='utf-8')
            logging.info(f"ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ ì €ì¥: {failed_file}")
        
        # ì§§ì€ íŒŒì¼ ëª©ë¡ ì €ì¥
        if self.conversion_results['short_files']:
            short_df = pd.DataFrame(self.conversion_results['short_files'])
            short_file = self.logs_path / 'short_files.csv'
            short_df.to_csv(short_file, index=False, encoding='utf-8')
            logging.info(f"ì§§ì€ íŒŒì¼ ëª©ë¡ ì €ì¥: {short_file}")
            
            # ì§§ì€ íŒŒì¼ ëª©ë¡ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œë„ ì €ì¥
            short_list_file = self.logs_path / 'short_files.txt'
            with open(short_list_file, 'w', encoding='utf-8') as f:
                f.write("=== í…ìŠ¤íŠ¸ê°€ 100ì ì´í•˜ì¸ íŒŒì¼ ëª©ë¡ ===\n\n")
                for file_info in self.conversion_results['short_files']:
                    f.write(f"íŒŒì¼ëª…: {file_info['file_name']}\n")
                    f.write(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {file_info['text_length']}ì\n")
                    f.write(f"íŒŒì¼ ê²½ë¡œ: {file_info['file_path']}\n")
                    f.write("-" * 50 + "\n")
            logging.info(f"ì§§ì€ íŒŒì¼ ëª©ë¡ í…ìŠ¤íŠ¸ ì €ì¥: {short_list_file}")
        
        # ì „ì²´ ê²°ê³¼ JSON ì €ì¥
        results_file = self.logs_path / 'conversion_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.conversion_results, f, ensure_ascii=False, indent=2, default=str)
        
        logging.info(f"ì „ì²´ ê²°ê³¼ ì €ì¥: {results_file}")
    
    def create_summary_report(self):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        logging.info("=== ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ===")
        
        stats = self.conversion_results['statistics']
        
        report = f"""# ë¬¸ì„œ ë³€í™˜ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š ë³€í™˜ í†µê³„
- **ì´ íŒŒì¼ ìˆ˜**: {stats['total_files']}ê°œ
- **ì„±ê³µ**: {stats['successful_files']}ê°œ
- **ì‹¤íŒ¨**: {stats['failed_files']}ê°œ
- **ì§§ì€ íŒŒì¼**: {stats['short_files']}ê°œ
- **ì„±ê³µë¥ **: {stats['success_rate']:.1f}%

## ğŸ“„ í…ìŠ¤íŠ¸ í†µê³„
- **í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´**: {stats['average_text_length']:.0f}ì
- **ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´**: {stats['min_text_length']}ì
- **ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´**: {stats['max_text_length']}ì

## ğŸ“ ìƒì„±ëœ íŒŒì¼
- **í…ìŠ¤íŠ¸ íŒŒì¼**: {stats['successful_files']}ê°œ (.txt)
- **ì„±ê³µ ëª©ë¡**: successful_conversions.csv
- **ì‹¤íŒ¨ ëª©ë¡**: failed_files.csv
- **ì§§ì€ íŒŒì¼ ëª©ë¡**: short_files.csv, short_files.txt
- **ì „ì²´ ê²°ê³¼**: conversion_results.json

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„
1. **ì§§ì€ íŒŒì¼ ê²€í† ** - 100ì ì´í•˜ íŒŒì¼ í’ˆì§ˆ í™•ì¸
2. **ì‹¤íŒ¨ íŒŒì¼ ë¶„ì„** - ë³€í™˜ ì‹¤íŒ¨ ì›ì¸ íŒŒì•…
3. **í…ìŠ¤íŠ¸ ì²­í‚¹** - RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì²­í‚¹ ìˆ˜í–‰
4. **FAISS ì„ë² ë”©** - ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•

---
ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        report_file = self.logs_path / 'conversion_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logging.info(f"ìš”ì•½ ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    def print_summary(self):
        """ë³€í™˜ ìš”ì•½ ì¶œë ¥"""
        stats = self.conversion_results['statistics']
        
        print("\n" + "="*60)
        print("ğŸ“Š ë¬¸ì„œ ë³€í™˜ ì™„ë£Œ ìš”ì•½")
        print("="*60)
        print(f"ì´ ì‹œë„ íŒŒì¼ ìˆ˜: {stats['total_files']}ê°œ")
        print(f"ì„±ê³µ íŒŒì¼ ìˆ˜: {stats['successful_files']}ê°œ")
        print(f"ì‹¤íŒ¨ íŒŒì¼ ìˆ˜: {stats['failed_files']}ê°œ")
        print(f"ì§§ì€ íŒŒì¼ ìˆ˜: {stats['short_files']}ê°œ")
        print(f"ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
        print(f"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['average_text_length']:.0f}ì")
        print("="*60)
        
        if stats['short_files'] > 0:
            print(f"\nâš ï¸  ì£¼ì˜: {stats['short_files']}ê°œ íŒŒì¼ì´ 100ì ì´í•˜ì…ë‹ˆë‹¤.")
            print("   conversion_logs/short_files.txt íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if stats['failed_files'] > 0:
            print(f"\nâŒ ì‹¤íŒ¨: {stats['failed_files']}ê°œ íŒŒì¼ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("   conversion_logs/failed_files.csv íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    def run_conversion(self):
        """ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logging.info("=== ë¬¸ì„œ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
        
        # 1. ë¬¸ì„œ ë³€í™˜
        self.convert_documents()
        
        # 2. ê²°ê³¼ ì €ì¥
        self.save_results()
        
        # 3. ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        self.create_summary_report()
        
        # 4. ìš”ì•½ ì¶œë ¥
        self.print_summary()
        
        logging.info("=== ë¬¸ì„œ ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ===")

if __name__ == "__main__":
    # ê²½ë¡œ ì„¤ì •
    input_path = r"C:\Users\user\Desktop\ì½”ë“œì‡ ìŠ¤í”„ë¦°íŠ¸1\AI ê°œë°œì\ë¯¸ì…˜ì œì¶œ ëª¨ìŒ\ì½”ë“œì‡ í”„ë¡œì íŠ¸ ëª¨ìŒ\ì¤‘ê¸‰ í”„ë¡œì íŠ¸\base_data\extracted_files-20250722T073504Z-1-001\files"
    output_path = r"C:\Users\user\Desktop\ì½”ë“œì‡ ìŠ¤í”„ë¦°íŠ¸1\AI ê°œë°œì\ë¯¸ì…˜ì œì¶œ ëª¨ìŒ\ì½”ë“œì‡ í”„ë¡œì íŠ¸ ëª¨ìŒ\ì¤‘ê¸‰ í”„ë¡œì íŠ¸\processed_data"
    
    # ë³€í™˜ê¸° ìƒì„± ë° ì‹¤í–‰
    converter = DocumentConverter(input_path, output_path)
    converter.run_conversion() 