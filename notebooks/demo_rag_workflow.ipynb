{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e6b7-b697-4b8a-b6f4-827f9a12a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# ✅ 환경 설정\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# ✅ 경로 설정\n",
    "CHUNKS_DIR = \"output_jsonl_chunks\"\n",
    "VECTOR_INDEX = \"vector.index\"\n",
    "VECTOR_METADATA = \"vector_metadata.json\"\n",
    "DATA_LIST = \"data_list.csv\"\n",
    "\n",
    "# ✅ 파일명 정규화 함수\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    name = Path(filename).stem\n",
    "    name = re.sub(r'[\\\\/:*?\"<>|()\\u3000\\s]+', '', name)\n",
    "    return name.strip()\n",
    "\n",
    "# ✅ 가장 유사한 파일명 찾기\n",
    "def find_closest_filename(target, candidates):\n",
    "    match = process.extractOne(target, candidates, scorer=fuzz.ratio)\n",
    "    if match and match[1] > 90:\n",
    "        return match[0]\n",
    "    return None\n",
    "\n",
    "# ✅ 메타데이터 enrich (data_list.csv 연동)\n",
    "def enrich_metadata(meta: dict, data_df: pd.DataFrame) -> dict:\n",
    "    fname = meta[\"filename\"].strip()\n",
    "    row = data_df[data_df[\"파일명\"].str.strip() == fname]\n",
    "    if not row.empty:\n",
    "        row = row.iloc[0]\n",
    "        for col in [\"공고 번호\", \"사업명\", \"사업 금액\", \"발주 기관\", \"입찰 참여 마감일\"]:\n",
    "            meta[col] = row.get(col, \"\")\n",
    "    return meta\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "index = faiss.read_index(VECTOR_INDEX)\n",
    "\n",
    "with open(VECTOR_METADATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    vector_metadatas = json.load(f)\n",
    "\n",
    "data_list = pd.read_csv(DATA_LIST)\n",
    "\n",
    "# ✅ 청크 파일 로드 (청크 단위로 title/subtitle/index 포함)\n",
    "all_chunks = {}\n",
    "loaded_files = set()\n",
    "actual_files = {sanitize_filename(f): f for f in os.listdir(CHUNKS_DIR)}\n",
    "\n",
    "for meta in vector_metadatas:\n",
    "    filename = meta[\"filename\"]\n",
    "    sanitized = sanitize_filename(filename)\n",
    "\n",
    "    if sanitized not in loaded_files:\n",
    "        file_match = actual_files.get(sanitized)\n",
    "        if not file_match:\n",
    "            file_match = find_closest_filename(sanitized, actual_files.keys())\n",
    "            if file_match:\n",
    "                file_match = actual_files[file_match]\n",
    "        if not file_match:\n",
    "            print(f\"❌ 파일 없음: {sanitized}.jsonl\")\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(CHUNKS_DIR, file_match), \"r\", encoding=\"utf-8\") as f:\n",
    "            all_chunks[sanitized] = [json.loads(line) for line in f]\n",
    "        loaded_files.add(sanitized)\n",
    "\n",
    "# ✅ 유사 청크 검색 함수 (context window + subtitle-aware)\n",
    "def search_similar_chunks(query, top_k, context_window=1):\n",
    "    response = openai.embeddings.create(input=[query], model=\"text-embedding-3-small\")\n",
    "    query_embedding = np.array(response.data[0].embedding).astype(\"float32\")\n",
    "\n",
    "    D, I = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "    for idx in I[0]:\n",
    "        if idx < 0 or idx >= len(vector_metadatas): continue\n",
    "        meta = vector_metadatas[idx]\n",
    "        meta = enrich_metadata(meta, data_list)\n",
    "        filename = meta[\"filename\"]\n",
    "        sanitized = sanitize_filename(filename)\n",
    "        base_idx = meta[\"index\"]\n",
    "\n",
    "        if sanitized not in all_chunks:\n",
    "            print(f\"❌ 청크 로딩 실패: {sanitized}\")\n",
    "            continue\n",
    "\n",
    "        chunk_list = all_chunks[sanitized]\n",
    "\n",
    "        for offset in range(-context_window, context_window + 1):\n",
    "            cidx = base_idx + offset\n",
    "            if 0 <= cidx < len(chunk_list) and (sanitized, cidx) not in seen:\n",
    "                seen.add((sanitized, cidx))\n",
    "                chunk = chunk_list[cidx]\n",
    "                results.append({\n",
    "                    \"text\": chunk[\"text\"],\n",
    "                    \"metadata\": {**meta, \"title\": chunk[\"title\"], \"subtitle\": chunk[\"subtitle\"], \"index\": chunk[\"index\"]}\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# ✅ GPT 답변 생성 (title/subtitle + 메타데이터 포함)\n",
    "def generate_answer(query, chunks):\n",
    "    context = \"\"\n",
    "    for c in chunks:\n",
    "        m = c[\"metadata\"]\n",
    "        context += f\"\"\"\n",
    "📄 [문서 정보]\n",
    "- 제목: {m.get('title', '')}\n",
    "- 소제목: {m.get('subtitle', '')}\n",
    "- 공고번호: {m.get('공고 번호', '')}\n",
    "- 발주기관: {m.get('발주 기관', '')}\n",
    "- 사업명: {m.get('사업명', '')}\n",
    "- 예산: {m.get('사업 금액', '')}\n",
    "- 마감일: {m.get('입찰 참여 마감일', '')}\n",
    "\n",
    "📑 [본문]\n",
    "{c['text']}\n",
    "\n",
    "\"\"\".strip() + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"다음은 사용자의 질문과 관련된 문서 내용입니다.\n",
    "\n",
    "[질문]\n",
    "{query}\n",
    "\n",
    "[문서]\n",
    "{context}\n",
    "\n",
    "위 문서들을 참고하여 질문에 대해 명확하고 간결하게 답변하세요.\"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ✅ 실행 예시\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"한국연구재단에서는 어떤 내용을 보냈어?\"\n",
    "    results = search_similar_chunks(query, top_k=20, context_window=1)\n",
    "\n",
    "    print(f\"🔍 검색된 청크 수: {len(results)}\")\n",
    "    for i, r in enumerate(results):\n",
    "        m = r[\"metadata\"]\n",
    "        print(f\"\\n🔹 {i+1}. {m['filename']} | {m['title']} > {m['subtitle']} | idx: {m['index']}\")\n",
    "        print(r['text'], \"...\")\n",
    "\n",
    "    print(\"\\n🧠 GPT 응답:\")\n",
    "    print(generate_answer(query, results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
