{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e6b7-b697-4b8a-b6f4-827f9a12a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# âœ… í™˜ê²½ ì„¤ì •\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "CHUNKS_DIR = \"output_jsonl_chunks\"\n",
    "VECTOR_INDEX = \"vector.index\"\n",
    "VECTOR_METADATA = \"vector_metadata.json\"\n",
    "DATA_LIST = \"data_list.csv\"\n",
    "\n",
    "# âœ… íŒŒì¼ëª… ì •ê·œí™” í•¨ìˆ˜\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    name = Path(filename).stem\n",
    "    name = re.sub(r'[\\\\/:*?\"<>|()\\u3000\\s]+', '', name)\n",
    "    return name.strip()\n",
    "\n",
    "# âœ… ê°€ì¥ ìœ ì‚¬í•œ íŒŒì¼ëª… ì°¾ê¸°\n",
    "def find_closest_filename(target, candidates):\n",
    "    match = process.extractOne(target, candidates, scorer=fuzz.ratio)\n",
    "    if match and match[1] > 90:\n",
    "        return match[0]\n",
    "    return None\n",
    "\n",
    "# âœ… ë©”íƒ€ë°ì´í„° enrich (data_list.csv ì—°ë™)\n",
    "def enrich_metadata(meta: dict, data_df: pd.DataFrame) -> dict:\n",
    "    fname = meta[\"filename\"].strip()\n",
    "    row = data_df[data_df[\"íŒŒì¼ëª…\"].str.strip() == fname]\n",
    "    if not row.empty:\n",
    "        row = row.iloc[0]\n",
    "        for col in [\"ê³µê³  ë²ˆí˜¸\", \"ì‚¬ì—…ëª…\", \"ì‚¬ì—… ê¸ˆì•¡\", \"ë°œì£¼ ê¸°ê´€\", \"ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼\"]:\n",
    "            meta[col] = row.get(col, \"\")\n",
    "    return meta\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ\n",
    "index = faiss.read_index(VECTOR_INDEX)\n",
    "\n",
    "with open(VECTOR_METADATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    vector_metadatas = json.load(f)\n",
    "\n",
    "data_list = pd.read_csv(DATA_LIST)\n",
    "\n",
    "# âœ… ì²­í¬ íŒŒì¼ ë¡œë“œ (ì²­í¬ ë‹¨ìœ„ë¡œ title/subtitle/index í¬í•¨)\n",
    "all_chunks = {}\n",
    "loaded_files = set()\n",
    "actual_files = {sanitize_filename(f): f for f in os.listdir(CHUNKS_DIR)}\n",
    "\n",
    "for meta in vector_metadatas:\n",
    "    filename = meta[\"filename\"]\n",
    "    sanitized = sanitize_filename(filename)\n",
    "\n",
    "    if sanitized not in loaded_files:\n",
    "        file_match = actual_files.get(sanitized)\n",
    "        if not file_match:\n",
    "            file_match = find_closest_filename(sanitized, actual_files.keys())\n",
    "            if file_match:\n",
    "                file_match = actual_files[file_match]\n",
    "        if not file_match:\n",
    "            print(f\"âŒ íŒŒì¼ ì—†ìŒ: {sanitized}.jsonl\")\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(CHUNKS_DIR, file_match), \"r\", encoding=\"utf-8\") as f:\n",
    "            all_chunks[sanitized] = [json.loads(line) for line in f]\n",
    "        loaded_files.add(sanitized)\n",
    "\n",
    "# âœ… ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ í•¨ìˆ˜ (context window + subtitle-aware)\n",
    "def search_similar_chunks(query, top_k, context_window=1):\n",
    "    response = openai.embeddings.create(input=[query], model=\"text-embedding-3-small\")\n",
    "    query_embedding = np.array(response.data[0].embedding).astype(\"float32\")\n",
    "\n",
    "    D, I = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "    for idx in I[0]:\n",
    "        if idx < 0 or idx >= len(vector_metadatas): continue\n",
    "        meta = vector_metadatas[idx]\n",
    "        meta = enrich_metadata(meta, data_list)\n",
    "        filename = meta[\"filename\"]\n",
    "        sanitized = sanitize_filename(filename)\n",
    "        base_idx = meta[\"index\"]\n",
    "\n",
    "        if sanitized not in all_chunks:\n",
    "            print(f\"âŒ ì²­í¬ ë¡œë”© ì‹¤íŒ¨: {sanitized}\")\n",
    "            continue\n",
    "\n",
    "        chunk_list = all_chunks[sanitized]\n",
    "\n",
    "        for offset in range(-context_window, context_window + 1):\n",
    "            cidx = base_idx + offset\n",
    "            if 0 <= cidx < len(chunk_list) and (sanitized, cidx) not in seen:\n",
    "                seen.add((sanitized, cidx))\n",
    "                chunk = chunk_list[cidx]\n",
    "                results.append({\n",
    "                    \"text\": chunk[\"text\"],\n",
    "                    \"metadata\": {**meta, \"title\": chunk[\"title\"], \"subtitle\": chunk[\"subtitle\"], \"index\": chunk[\"index\"]}\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# âœ… GPT ë‹µë³€ ìƒì„± (title/subtitle + ë©”íƒ€ë°ì´í„° í¬í•¨)\n",
    "def generate_answer(query, chunks):\n",
    "    context = \"\"\n",
    "    for c in chunks:\n",
    "        m = c[\"metadata\"]\n",
    "        context += f\"\"\"\n",
    "ğŸ“„ [ë¬¸ì„œ ì •ë³´]\n",
    "- ì œëª©: {m.get('title', '')}\n",
    "- ì†Œì œëª©: {m.get('subtitle', '')}\n",
    "- ê³µê³ ë²ˆí˜¸: {m.get('ê³µê³  ë²ˆí˜¸', '')}\n",
    "- ë°œì£¼ê¸°ê´€: {m.get('ë°œì£¼ ê¸°ê´€', '')}\n",
    "- ì‚¬ì—…ëª…: {m.get('ì‚¬ì—…ëª…', '')}\n",
    "- ì˜ˆì‚°: {m.get('ì‚¬ì—… ê¸ˆì•¡', '')}\n",
    "- ë§ˆê°ì¼: {m.get('ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼', '')}\n",
    "\n",
    "ğŸ“‘ [ë³¸ë¬¸]\n",
    "{c['text']}\n",
    "\n",
    "\"\"\".strip() + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤.\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{query}\n",
    "\n",
    "[ë¬¸ì„œ]\n",
    "{context}\n",
    "\n",
    "ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# âœ… ì‹¤í–‰ ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"í•œêµ­ì—°êµ¬ì¬ë‹¨ì—ì„œëŠ” ì–´ë–¤ ë‚´ìš©ì„ ë³´ëƒˆì–´?\"\n",
    "    results = search_similar_chunks(query, top_k=20, context_window=1)\n",
    "\n",
    "    print(f\"ğŸ” ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(results)}\")\n",
    "    for i, r in enumerate(results):\n",
    "        m = r[\"metadata\"]\n",
    "        print(f\"\\nğŸ”¹ {i+1}. {m['filename']} | {m['title']} > {m['subtitle']} | idx: {m['index']}\")\n",
    "        print(r['text'], \"...\")\n",
    "\n",
    "    print(\"\\nğŸ§  GPT ì‘ë‹µ:\")\n",
    "    print(generate_answer(query, results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
